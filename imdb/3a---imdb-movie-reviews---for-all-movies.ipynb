{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import urllib\n",
    "import urllib2\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "hdr = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "    'Accept-Encoding': 'none',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Connection': 'keep-alive'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------- #\n",
    "def getUrlSoup(url,hdr):\n",
    "    req1 = urllib2.Request(url, headers=hdr)\n",
    "    page = urllib2.urlopen(req1)\n",
    "    content = page.read()\n",
    "    soup = BeautifulSoup(content)\n",
    "    return soup\n",
    "    # --------------------------------------------- #\n",
    "\n",
    "\n",
    "# --------------------------------------------- #\n",
    "def getReviews(main_block, reviews_obj=None):\n",
    "    \n",
    "    assert reviews_obj is not None\n",
    "    \n",
    "    # Treat it as a massive block of text and split it on the horizontal lines.\n",
    "    reviews_block = re.split(r\"<hr[^>]*>\", str(main_block))\n",
    "    \n",
    "    # Iterate over all the reviews.\n",
    "    # NOTE: The indexing is dubious here...\n",
    "    for r in reviews_block[1:-3]:\n",
    "        \n",
    "        # Soup the block of html-code-text\n",
    "        soup_review = BeautifulSoup(r)\n",
    "        soup_review_header = soup_review.find('div')\n",
    "        \n",
    "        # Create an empty dictionary to hold the review.\n",
    "        review = dict()\n",
    "        \n",
    "        # Get the user-name, user-link for the reviewer.\n",
    "        review['user_name'] = soup_review_header.findAll('a')[1].get_text()\n",
    "        review['user_href'] = soup_review_header.findAll('a')[1]['href']\n",
    "\n",
    "        # Try to get the rating.\n",
    "        soup_imgs = soup_review_header.findAll('img')\n",
    "        review['review_rating'] = None\n",
    "        if len(soup_imgs) > 1:\n",
    "            review['review_rating'] = int(soup_imgs[1]['alt'].split('/')[0])\n",
    "        \n",
    "        # Get the review title.\n",
    "        review['review_title'] = soup_review.find('h2').get_text()\n",
    "        \n",
    "        # Get the review content.\n",
    "        tmp_content = soup_review.findAll('p')\n",
    "        if len(tmp_content) > 1 and 'spoilers' in tmp_content[0].get_text():\n",
    "            review['review_content'] = tmp_content[1].get_text()\n",
    "        else:\n",
    "            review['review_content'] = tmp_content[0].get_text()\n",
    "        \n",
    "        # Get the review usefulness.\n",
    "        # review['review_useful'] = soup_review_header.find('small').get_text().split('people')[0].rstrip().lstrip()\n",
    "        \n",
    "        reviews_obj.append(review)\n",
    "        \n",
    "    # Done. Return the reviews_obj\n",
    "    return reviews_obj\n",
    "    # --------------------------------------------- #\n",
    "\n",
    "\n",
    "# --------------------------------------------- #\n",
    "def fnShowNumberOfReviews(MOVIES):\n",
    "    s = 0\n",
    "    for m in MOVIES:\n",
    "        if 'reviews' in m:\n",
    "            # print m['name'], len(m['reviews'])\n",
    "            s += len(m['reviews'])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return s\n",
    "    # --------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the list of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9869 movies in the dataset.\n",
      "\n",
      "Number of reviews available: 612354\n"
     ]
    }
   ],
   "source": [
    "MOVIES = json.load(open('../../data-blog---list_of_movies_v3-v1200.json','r'))\n",
    "\n",
    "print 'We have {} movies in the dataset.'.format(len(MOVIES))\n",
    "\n",
    "print '\\nNumber of reviews available: {}'.format(fnShowNumberOfReviews(MOVIES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each movie we will grab the ID and get the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing reviews for all movies...\n",
      "\n",
      "\t SKIPPING --->  Till We Meet Again 1001 . 1002 . 1003 . 1004 . 1005 . 1006 . 1007 . 1008 . 1009 . 1010 . 1011 . 1012 . 1013 . 1014 . 1015 . 1016 . 1017 . 1018 . 1019 . 1020 . 1021 . 1022 . 1023 . 1024 . 1025 . 1026 . 1027 . 1028 . 1029 . 1030 . 1031 . 1032 . 1033 . 1034 . 1035 . 1036 . 1037 . 1038 . 1039 . 1040 . 1041 . 1042 . 1043 . 1044 . 1045 . 1046 . 1047 . 1048 . 1049 . 1050 . 1051 . 1052 . 1053 . 1054 . 1055 . 1056 . 1057 . 1058 . 1059 . 1060 . 1061 . 1062 . 1063 . 1064 . 1065 . 1066 . 1067 . 1068 . 1069 . 1070 . 1071 . 1072 . 1073 . 1074 . 1075 . 1076 . 1077 . 1078 . 1079 . 1080 . 1081 . 1082 . 1083 . 1084 . 1085 . 1086 . 1087 . 1088 . 1089 . 1090 . 1091 . 1092 . 1093 . 1094 . 1095 . 1096 . 1097 . 1098 . 1099 . 1100 . 1101 . 1102 . 1103 . 1104 . 1105 . 1106 . 1107 . 1108 . 1109 . 1110 . 1111 . 1112 . 1113 . 1114 . 1115 . 1116 . 1117 . 1118 . 1119 . 1120 . 1121 . 1122 . 1123 . 1124 . 1125 . 1126 . 1127 . 1128 . 1129 . 1130 . 1131 . 1132 . 1133 . 1134 . 1135 . 1136 . 1137 . 1138 . 1139 . 1140 . 1141 . 1142 . 1143 . 1144 . 1145 . 1146 . 1147 . 1148 . 1149 . 1150 . 1151 . 1152 . 1153 . 1154 . 1155 . 1156 . 1157 . 1158 . 1159 . 1160 . 1161 . 1162 . 1163 . 1164 . 1165 . 1166 . 1167 . 1168 . 1169 . 1170 . 1171 . 1172 . 1173 . 1174 . 1175 . 1176 . 1177 . 1178 . 1179 . 1180 . 1181 . 1182 . 1183 . 1184 . 1185 . 1186 . 1187 . 1188 . 1189 . 1190 . 1191 . 1192 . 1193 . 1194 . 1195 . 1196 . 1197 . 1198 . 1199 . \t --->  Can't Hardly Wait  -- pages:  25\n",
      "1200 . \t --->  Lethal Weapon 4  -- pages:  29\n",
      "1201 . \t --->  Beloved  -- pages:  21\n",
      "1202 . \t --->  Ever After: A Cinderella Story  -- pages:  35\n",
      "1203 . \t --->  Rushmore  -- pages:  64\n",
      "1204 . \t --->  The Siege  -- pages:  28\n",
      "1205 . \t --->  The 13th Warrior  -- pages:  55\n",
      "1206 . \t --->  Blast from the Past  -- pages:  24\n",
      "1207 . \t --->  Stuart Little  -- pages:  18\n",
      "1208 . \t --->  Varsity Blues  -- pages:  27\n",
      "1209 . \t --->  Scream 3  -- pages:  73\n",
      "1210 . \t --->  Hollow Man  -- pages:  63\n",
      "1211 . \t --->  The Legend of Bagger Vance  -- pages:  27\n",
      "1212 . \t --->  The Perfect Storm  -- pages:  78\n",
      "1213 . \t --->  U-571  -- pages:  60\n",
      "1214 . \t --->  What Women Want  -- pages:  40\n",
      "1215 . \t --->  From Hell  -- pages:  54\n",
      "1216 . \t --->  Joe Dirt  -- pages:  21\n",
      "1217 . \t --->  Serendipity  -- pages:  37\n",
      "1218 . \t --->  Spy Game  -- pages:  36\n",
      "1219 . \t --->  John Q  -- pages:  40\n",
      "1220 . \t --->  Reign of Fire  -- pages:  57\n",
      "1221 . \t --->  The Scorpion King  -- pages:  44\n",
      "1222 . \t --->  The Time Machine  -- pages:  62\n",
      "1223 . \t --->  Charlie's Angels: Full Throttle  -- pages:  56\n",
      "1224 . \t --->  Freddy vs. Jason  -- pages:  88\n",
      "1225 . \t --->  Secondhand Lions  -- pages:  29\n",
      "1226 . \t --->  Under the Tuscan Sun  -- pages:  32\n",
      "1227 . \t --->  Mona Lisa Smile  -- pages:  27\n",
      "1228 . \t --->  Timeline  -- pages:  46\n",
      "1229 . \t --->  Confessions of a Teenage Drama Queen  -- pages:  11\n",
      "1230 . \t --->  The Life Aquatic with Steve Zissou  -- pages:  63\n",
      "1231 . \t --->  The Manchurian Candidate  -- pages:  36\n",
      "1232 . \t --->  Raising Helen  -- pages:  11\n",
      "1233 . \t --->  Spanglish  -- pages:  34\n",
      "1234 . \t --->  Kung Fu Hustle  -- pages:  36\n",
      "1235 . \t --->  Æon Flux  -- pages:  53\n",
      "1236 . \t --->  The Brothers Grimm  -- pages:  49\n",
      "1237 . \t --->  The Constant Gardener  -- pages:  57\n",
      "1238 . \t --->  Derailed  -- pages:  38\n",
      "1239 . \t --->  Doom  -- pages:  81\n",
      "1240 . \t --->  The Exorcism of Emily Rose  -- pages:  52\n",
      "1241 . \t --->  Red Eye  -- pages:  67\n",
      "1242 . \t --->  Rent  -- pages:  76\n",
      "1243 . \t --->  Rumor Has It...  -- pages:  19\n",
      "1244 . \t --->  The Skeleton Key  -- pages:  37\n",
      "1245 . \t --->  Sky High  -- pages:  22\n",
      "1246 . \t --->  Zathura: A Space Adventure  -- pages:  21\n",
      "1247 . \t --->  Big Momma's House 2  -- pages:  9\n",
      "1248 . \t --->  The Black Dahlia  -- pages:  65\n",
      "1249 . \t --->  The Break-Up  -- pages:  49\n",
      "1250 . \t --->  Clerks II  -- pages:  48\n",
      "1251 . \t --->  Final Destination 3  -- pages:  57\n",
      "1252 . \t --->  Just My Luck  -- pages:  15\n",
      "1253 . \t --->  The Lake House  -- pages:  55\n",
      "1254 . \t --->  Nanny McPhee  -- pages:  19\n",
      "1255 . \t --->  Open Season  -- pages:  10\n",
      "1256 . \t --->  Running Scared  -- pages:  42\n",
      "1257 . \t --->  Saw III  -- pages:  62\n",
      "1258 . \t --->  Slither  -- pages:  36\n",
      "1259 . \t --->  Tenacious D in The Pick of Destiny  -- pages:  20\n",
      "1260 . \t --->  Charlotte's Web  -- pages:  10\n",
      "1261 . \t --->  A Good Year  -- pages:  19\n",
      "1262 . \t --->  AVPR: Aliens vs Predator - Requiem  -- pages:  100\n",
      "1263 . \t --->  Black Snake Moan  -- pages:  20\n",
      "1264 . \t --->  The Brave One  -- pages:  29\n",
      "1265 . \t --->  Dan in Real Life  -- pages:  27\n",
      "1266 . \t --->  The Darjeeling Limited  -- pages:  28\n",
      "1267 . \t --->  The Game Plan  -- pages:  9\n",
      "1268 . \t --->  The Heartbreak Kid  -- pages:  18\n",
      "1269 . \t --->  I Now Pronounce You Chuck & Larry  -- pages:  23\n",
      "1270 . \t --->  License to Wed  -- pages:  12\n",
      "1271 . \t --->  Norbit  -- pages:  27\n",
      "1272 . \t --->  Sydney White  -- pages:  5\n",
      "1273 . \t --->  We Own the Night  -- pages:  20\n",
      "1274 . \t --->  Wild Hogs  -- pages:  26\n",
      "1275 . \t --->  The Orphanage  -- pages:  31\n",
      "1276 . \t --->  Semi-Pro  -- pages:  12\n",
      "1277 . \t --->  Untraceable  -- pages:  25\n",
      "1278 . \t --->  Blindness  -- pages:  30\n",
      "1279 . \t --->  City of Ember  -- pages:  11\n",
      "1280 . \t --->  Doubt  -- pages:  32\n",
      "1281 . \t --->  Mirrors  -- pages:  24\n",
      "1282 . \t --->  Punisher: War Zone  -- pages:  28\n",
      "1283 . \t --->  Saw V  -- pages:  27\n",
      "1284 . \t --->  The Sisterhood of the Traveling Pants 2  -- pages:  4\n",
      "1285 . \t --->  Speed Racer  -- pages:  42\n",
      "1286 . \t --->  The Spirit  -- pages:  29\n",
      "1287 . \t --->  Traitor  -- pages:  13\n",
      "1288 . \t --->  12 Rounds  -- pages:  12\n",
      "1289 . \t --->  Brüno  -- pages:  40\n",
      "1290 . \t --->  Dragonball Evolution  -- pages:  53\n",
      "1291 . \t --->  The Final Destination  -- pages:  29\n",
      "1292 . \t --->  Fired Up!  -- pages:  7\n",
      "1293 . \t --->  The Goods: Live Hard, Sell Hard  -- pages:  6\n",
      "1294 . \t --->  I Love You, Beth Cooper  -- pages:  8\n",
      "1295 . \t --->  My Bloody Valentine  -- pages:  27\n",
      "1296 . \t --->  Ponyo  -- pages:  15\n",
      "1297 . \t --->  The Box  -- pages:  38\n",
      "1298 . \t --->  A Christmas Carol  -- pages:  25\n",
      "1299 . \t --->  Nine  -- pages:  24\n",
      "1300 . \t --->  Planet 51  -- pages:  8\n",
      "1301 . \t --->  Whip It  -- pages:  12\n",
      "1302 . \t --->  Country Strong  -- pages:  12\n",
      "1303 . \t --->  Death at a Funeral  -- pages:  10\n",
      "1304 . \t --->  Jonah Hex  -- pages:  18\n",
      "1305 . \t --->  Saw 3D  -- pages:  28\n",
      "1306 . \t --->  Yogi Bear  -- pages:  10\n",
      "1307 . \t --->  Don't Be Afraid of the Dark  -- pages:  25\n",
      "1308 . \t --->  Mars Needs Moms  -- pages:  12\n",
      "1309 . \t --->  Sound City  -- pages:  4\n",
      "1310 . \t --->  The Secret Life of Walter Mitty  -- pages:  50\n",
      "1311 . \t --->  Paranoia  -- pages:  8\n",
      "1312 . \t --->  The Babymakers  -- pages:  2\n",
      "1313 . \t --->  Citadel  -- pages:  4\n",
      "1314 . \t --->  Celebrity Sex Tape"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luuk/pyEnv/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b07b66303dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Get the number of pages to get reviews from.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         num_pages = int(main_block.findAll('table')[1]\n\u001b[0;32m---> 26\u001b[0;31m                                   .find('td',attrs={'nowrap':'1'}).get_text().split(' of ')[1].rstrip(':'))\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mMOVIES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_reviews_pages'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "print 'Grabbing reviews for all movies...\\n'\n",
    "\n",
    "added = 0\n",
    "\n",
    "# Iterate over all movies.\n",
    "for e,movie in enumerate(MOVIES[:1400]):\n",
    "    \n",
    "    if e == 282 or e == 1314:\n",
    "        print '\\t SKIPPING ---> ', movie['name'],\n",
    "    \n",
    "    # Check if the movie already has the reviews populated\n",
    "    elif 'reviews' not in movie:\n",
    "        \n",
    "        print '\\t ---> ', movie['name'],\n",
    "    \n",
    "        url = 'http://www.imdb.com' + movie['link'] + 'reviews'\n",
    "\n",
    "        # Get the contents for the first reviews page.\n",
    "        soup = getUrlSoup(url,hdr)\n",
    "        # Get the main content block.\n",
    "        main_block = (soup.body.find('div', attrs={'class':'reviews'})\n",
    "                               .find('div', attrs={'id':'tn15main'})\n",
    "                               .find('div', attrs={'id':'tn15content'}))\n",
    "        # Get the number of pages to get reviews from.\n",
    "        num_pages = int(main_block.findAll('table')[1]\n",
    "                                  .find('td',attrs={'nowrap':'1'}).get_text().split(' of ')[1].rstrip(':'))\n",
    "\n",
    "        MOVIES[e]['num_reviews_pages'] = num_pages\n",
    "        print ' -- pages: ', num_pages\n",
    "\n",
    "        # Iterate over all the reviews pages.\n",
    "        REVIEWS = []\n",
    "\n",
    "        for i in range(1,num_pages+1):\n",
    "\n",
    "            # If this is the first page, we don't need to load the new page.\n",
    "            if i == 1:\n",
    "                REVIEWS = getReviews(main_block,REVIEWS)\n",
    "\n",
    "            # We need to grab the contents for the next page.\n",
    "            else:\n",
    "                page_soup = getUrlSoup(url + '?start=' + str(10*(i-1)), hdr)\n",
    "                main_block = (page_soup.body.find('div', attrs={'class':'reviews'})\n",
    "                                            .find('div', attrs={'id':'tn15main'})\n",
    "                                            .find('div', attrs={'id':'tn15content'}))\n",
    "                REVIEWS = getReviews(main_block,REVIEWS)\n",
    "\n",
    "        # Done for this movie. Add reviews to the movie object.\n",
    "        MOVIES[e]['reviews'] = REVIEWS\n",
    "        added += len(REVIEWS)\n",
    "\n",
    "\n",
    "    # If the review is there. Sanity check...\n",
    "    # else:\n",
    "#         # nothing.\n",
    "#         url = 'http://www.imdb.com' + movie['link'] + 'reviews'\n",
    "#         soup = getUrlSoup(url,hdr)\n",
    "#         # Get the main content block.\n",
    "#         main_block = (soup.body.find('div', attrs={'class':'reviews'})\n",
    "#                                .find('div', attrs={'id':'tn15main'})\n",
    "#                                .find('div', attrs={'id':'tn15content'}))\n",
    "#         # Get the number of pages to get reviews from.\n",
    "#         num_pages = int(main_block.findAll('table')[1]\n",
    "#                                   .find('td',attrs={'nowrap':'1'}).get_text().split(' of ')[1].rstrip(':'))\n",
    "#         if \n",
    "        \n",
    "    if e % 1 == 0 and e > 1000:\n",
    "        print e, '.',\n",
    "        \n",
    "\n",
    "print 'DONE! Added {} reviews'.format(added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of reviews available: 648988\n"
     ]
    }
   ],
   "source": [
    "print 'Number of reviews available: {}'.format(fnShowNumberOfReviews(MOVIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Full dump\n",
    "with open('../../data-blog---list_of_movies_v3-v1313.json', 'w') as outfile:\n",
    "    json.dump(MOVIES, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Dump JSONL line by line\n",
    "# with open('../../list_of_movies_with_reviews.json', 'a') as the_file:\n",
    "# the_file.write('Hello\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
